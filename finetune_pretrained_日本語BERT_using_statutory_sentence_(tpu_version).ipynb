{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune pretrained 日本語BERT using statutory sentence (tpu version)",
      "provenance": [],
      "collapsed_sections": [
        "w74xVsUKOdw0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/typijo/bert-tltc/blob/master/finetune_pretrained_%E6%97%A5%E6%9C%AC%E8%AA%9EBERT_using_statutory_sentence_(tpu_version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u8YfGE15Avu",
        "colab_type": "text"
      },
      "source": [
        "You need your GCP storage bucket to be set as follows:\n",
        "```\n",
        "/(backet name)/\n",
        "  pretrain_statutory_sentence/\n",
        "    (training data).txt # training data (see below)\n",
        "    vocab.txt # vocabulary (one line one word)\n",
        "  bert/\n",
        "    Japanese_L-12_H-768_A-12_E-30_BPE/\n",
        "      bert_model.ckpt.index\n",
        "      bert_model.ckpt.meta\n",
        "      bert_model.ckpt.data-00000-of-00001\t\n",
        "      bert_config.json\n",
        "```\n",
        "\n",
        "Format of the training data\n",
        "- one line one sentence\n",
        "- documents are separated by one blank line\n",
        "\n",
        "**example**\n",
        "```\n",
        "doc1-line1-word1 doc1-line1-word2 ...\n",
        "doc1-line2-word1 doc1-line2-word2 ...\n",
        "...\n",
        "\n",
        "doc2-line1-word1 doc2-line1-word2 ...\n",
        "doc2-line2-word1 doc2-line2-word2 ...\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB1LFFz3dUrf",
        "colab_type": "code",
        "outputId": "ea7d4541-e4ea-49e2-bfe8-601d7b9d5ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0801 03:43:12.355468 139677679544192 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laIIB3tOEQG-",
        "colab_type": "code",
        "outputId": "82d91137-59c2-4c17-ee67-8445686e38cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# load BERT codes\n",
        "import sys\n",
        "\n",
        "!test -d bert-tltc || git clone https://github.com/typijo/bert-tltc.git #changed to my repo\n",
        "if not 'bert-tltc' in sys.path:\n",
        "  sys.path += ['bert-tltc']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert-tltc'...\n",
            "remote: Enumerating objects: 553, done.\u001b[K\n",
            "Receiving objects:   0% (1/553)   \rReceiving objects:   1% (6/553)   \rReceiving objects:   2% (12/553)   \rReceiving objects:   3% (17/553)   \rReceiving objects:   4% (23/553)   \rReceiving objects:   5% (28/553)   \rReceiving objects:   6% (34/553)   \rReceiving objects:   7% (39/553)   \rReceiving objects:   8% (45/553)   \rReceiving objects:   9% (50/553)   \rReceiving objects:  10% (56/553)   \rReceiving objects:  11% (61/553)   \rReceiving objects:  12% (67/553)   \rReceiving objects:  13% (72/553)   \rReceiving objects:  14% (78/553)   \rReceiving objects:  15% (83/553)   \rReceiving objects:  16% (89/553)   \rReceiving objects:  17% (95/553)   \rReceiving objects:  18% (100/553)   \rReceiving objects:  19% (106/553)   \rReceiving objects:  20% (111/553)   \rReceiving objects:  21% (117/553)   \rReceiving objects:  22% (122/553)   \rReceiving objects:  23% (128/553)   \rReceiving objects:  24% (133/553)   \rReceiving objects:  25% (139/553)   \rReceiving objects:  26% (144/553)   \rReceiving objects:  27% (150/553)   \rReceiving objects:  28% (155/553)   \rReceiving objects:  29% (161/553)   \rReceiving objects:  30% (166/553)   \rReceiving objects:  31% (172/553)   \rReceiving objects:  32% (177/553)   \rReceiving objects:  33% (183/553)   \rReceiving objects:  34% (189/553)   \rReceiving objects:  35% (194/553)   \rReceiving objects:  36% (200/553)   \rReceiving objects:  37% (205/553)   \rReceiving objects:  38% (211/553)   \rReceiving objects:  39% (216/553)   \rReceiving objects:  40% (222/553)   \rReceiving objects:  41% (227/553)   \rReceiving objects:  42% (233/553)   \rReceiving objects:  43% (238/553)   \rReceiving objects:  44% (244/553)   \rReceiving objects:  45% (249/553)   \rReceiving objects:  46% (255/553)   \rReceiving objects:  47% (260/553)   \rReceiving objects:  48% (266/553)   \rReceiving objects:  49% (271/553)   \rReceiving objects:  50% (277/553)   \rReceiving objects:  51% (283/553)   \rReceiving objects:  52% (288/553)   \rReceiving objects:  53% (294/553)   \rReceiving objects:  54% (299/553)   \rReceiving objects:  55% (305/553)   \rReceiving objects:  56% (310/553)   \rReceiving objects:  57% (316/553)   \rReceiving objects:  58% (321/553)   \rReceiving objects:  59% (327/553)   \rReceiving objects:  60% (332/553)   \rReceiving objects:  61% (338/553)   \rReceiving objects:  62% (343/553)   \rReceiving objects:  63% (349/553)   \rReceiving objects:  64% (354/553)   \rReceiving objects:  65% (360/553)   \rReceiving objects:  66% (365/553)   \rReceiving objects:  67% (371/553)   \rReceiving objects:  68% (377/553)   \rReceiving objects:  69% (382/553)   \rReceiving objects:  70% (388/553)   \rReceiving objects:  71% (393/553)   \rremote: Total 553 (delta 0), reused 0 (delta 0), pack-reused 553\u001b[K\n",
            "Receiving objects:  72% (399/553)   \rReceiving objects:  73% (404/553)   \rReceiving objects:  74% (410/553)   \rReceiving objects:  75% (415/553)   \rReceiving objects:  76% (421/553)   \rReceiving objects:  77% (426/553)   \rReceiving objects:  78% (432/553)   \rReceiving objects:  79% (437/553)   \rReceiving objects:  80% (443/553)   \rReceiving objects:  81% (448/553)   \rReceiving objects:  82% (454/553)   \rReceiving objects:  83% (459/553)   \rReceiving objects:  84% (465/553)   \rReceiving objects:  85% (471/553)   \rReceiving objects:  86% (476/553)   \rReceiving objects:  87% (482/553)   \rReceiving objects:  88% (487/553)   \rReceiving objects:  89% (493/553)   \rReceiving objects:  90% (498/553)   \rReceiving objects:  91% (504/553)   \rReceiving objects:  92% (509/553)   \rReceiving objects:  93% (515/553)   \rReceiving objects:  94% (520/553)   \rReceiving objects:  95% (526/553)   \rReceiving objects:  96% (531/553)   \rReceiving objects:  97% (537/553)   \rReceiving objects:  98% (542/553)   \rReceiving objects:  99% (548/553)   \rReceiving objects: 100% (553/553)   \rReceiving objects: 100% (553/553), 317.33 KiB | 4.02 MiB/s, done.\n",
            "Resolving deltas:   0% (0/338)   \rResolving deltas:   1% (5/338)   \rResolving deltas:   2% (7/338)   \rResolving deltas:  13% (44/338)   \rResolving deltas:  14% (49/338)   \rResolving deltas:  25% (85/338)   \rResolving deltas:  28% (97/338)   \rResolving deltas:  30% (104/338)   \rResolving deltas:  32% (109/338)   \rResolving deltas:  36% (125/338)   \rResolving deltas:  38% (129/338)   \rResolving deltas:  40% (137/338)   \rResolving deltas:  42% (143/338)   \rResolving deltas:  50% (170/338)   \rResolving deltas:  62% (210/338)   \rResolving deltas:  74% (253/338)   \rResolving deltas:  98% (333/338)   \rResolving deltas: 100% (338/338)   \rResolving deltas: 100% (338/338), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiggCnuDFqaa",
        "colab_type": "text"
      },
      "source": [
        "### tfrecordの作成\n",
        "\n",
        "parameters are inherited from 日本語BERT and original bert\n",
        "https://github.com/google-research/bert#pre-training-with-bert\n",
        "\n",
        "- vocab is from 日本語BERT\n",
        "- inherition exception: dupe_factor here is 3 not 5 that is the original one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaddaKxlESiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your bucket where the textfile is\n",
        "BUCKET = \"japan-legal-term-correction\" #@param {type:\"string\"}\n",
        "\n",
        "# textfile\n",
        "TEXTFILE_NAME = \"corpus_elaws_1500.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# suffix for management\n",
        "SUFFIX_TFRECORD = \"1500\" #@param {type:\"string\"}\n",
        "\n",
        "DATASET_PATH = \"gs://{}/pretrain_statutory_sentence\".format(BUCKET)\n",
        "\n",
        "TEXT_FILE = \"{}/{}\".format(DATASET_PATH, TEXTFILE_NAME)\n",
        "TFRECORD_FILE = \"{}/tf_examples{}.tfrecord\".format(DATASET_PATH, SUFFIX_TFRECORD)\n",
        "VOCAB_FILE = \"{}/vocab.txt\".format(DATASET_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w74xVsUKOdw0",
        "colab_type": "text"
      },
      "source": [
        "#### 訓練データの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoZAgRcBJrL_",
        "colab_type": "code",
        "outputId": "7e27e354-ac5a-4bf9-d891-ec3015d04551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%env TEXT_FILE={TEXT_FILE}\n",
        "%env TFRECORD_FILE={TFRECORD_FILE}\n",
        "%env VOCAB_FILE={VOCAB_FILE}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: TEXT_FILE=gs://japan-legal-term-correction/pretrain_statutory_sentence/corpus_elaws_1500.txt\n",
            "env: TFRECORD_FILE=gs://japan-legal-term-correction/pretrain_statutory_sentence/tf_examples1500.tfrecord\n",
            "env: VOCAB_FILE=gs://japan-legal-term-correction/pretrain_statutory_sentence/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scqwnlPmGtDq",
        "colab_type": "code",
        "outputId": "15331983-3211-49d4-a20a-3825a0538849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "python bert-tltc/create_pretraining_data.py \\\n",
        "--input_file=${TEXT_FILE} --output_file=${TFRECORD_FILE} --vocab_file=${VOCAB_FILE} \\\n",
        "--do_lower_case=False --tokenize_chinese_chars=False --max_seq_length=128 --max_predictions_per_seq=20 --masked_lm_prob=0.15 --random_seed=0 --dupe_factor=3 2>&1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0801 03:45:05.082638 139981399807872 deprecation_wrapper.py:119] From bert-tltc/create_pretraining_data.py:446: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0801 03:45:05.083896 139981399807872 deprecation_wrapper.py:119] From bert-tltc/create_pretraining_data.py:414: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0801 03:45:05.084145 139981399807872 deprecation_wrapper.py:119] From bert-tltc/create_pretraining_data.py:414: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0801 03:45:05.084367 139981399807872 deprecation_wrapper.py:119] From /content/bert-tltc/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0801 03:45:06.985617 139981399807872 deprecation_wrapper.py:119] From bert-tltc/create_pretraining_data.py:421: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0801 03:45:07.173607 139981399807872 deprecation_wrapper.py:119] From bert-tltc/create_pretraining_data.py:423: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0801 03:45:07.173899 139981399807872 create_pretraining_data.py:423] *** Reading from input files ***\n",
            "I0801 03:45:07.174005 139981399807872 create_pretraining_data.py:425]   gs://japan-legal-term-correction/pretrain_statutory_sentence/corpus_elaws_1500.txt\n",
            "I0801 03:51:17.536370 139981399807872 create_pretraining_data.py:434] *** Writing to output files ***\n",
            "I0801 03:51:17.536761 139981399807872 create_pretraining_data.py:436]   gs://japan-legal-term-correction/pretrain_statutory_sentence/tf_examples1500.tfrecord\n",
            "W0801 03:51:17.537043 139981399807872 deprecation_wrapper.py:119] From bert-tltc/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0801 03:51:17.538670 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.539013 139981399807872 create_pretraining_data.py:151] tokens: [CLS] [MASK] 項 [MASK] 主張 を 明らか に す [MASK] 期限 及び 前 ##項 の 請求 の 期限 を 定める こと が できる 。 [MASK] [MASK] 又は 弁護 人 は [MASK] [MASK] ##条 第 二 項 の 規定 に ##より ミネソタ ##調 ##べ を 請求 し た 証拠 に [MASK] は 、 速やかに 、 検察 ##官 に [MASK] ##し 、 次 の 各 号 [SEP] 第 九 十 八 号 に 掲げる 事務 の うち 個別 労働 関係 紛争 の 解決 [MASK] 促進 に ##関 [MASK] 法律 （ 平成 十 三 [MASK] 法律 第 百 十 二 号 ） 及び 雇用 の 分野 [MASK] ##お [MASK] 男女 [MASK] 均 ##等 [MASK] 機会 及び 待遇 の 確保 [MASK] に ##関 ##する 法律 （ 昭和 四 十 七 年 [SEP]\n",
            "I0801 03:51:17.539205 139981399807872 create_pretraining_data.py:161] input_ids: 2 4 1235 4 862 10 10092 8 954 4 4767 388 134 17126 5 5111 5 4767 10 6010 30 11 146 7 4 4 4088 2663 50 9 4 4 5005 55 129 1235 5 1333 8 17450 10411 5558 3979 10 5111 31 1426 4129 8 4 9 6 17928 6 6465 5198 8 4 440 6 244 5 286 144 3 55 2205 1677 1148 144 8 12560 739 5 381 6461 759 250 4701 5 2649 4 4688 8 6568 4 1442 16 232 1677 168 4 1442 55 2659 1677 129 144 17 388 4413 5 1300 4 2920 4 3752 4 11495 5038 4 2224 388 12246 5 2005 4 8 6568 3244 1442 16 155 535 1677 1470 14 3\n",
            "I0801 03:51:17.539364 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.539515 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.539608 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 1 3 9 22 24 25 30 31 40 49 57 81 85 91 103 105 107 110 116 0\n",
            "I0801 03:51:17.539702 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 70 5 1450 146 14129 50 6 134 4714 19591 15994 5 3244 14 8 3316 5 276 174 0\n",
            "I0801 03:51:17.539805 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.539884 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.540457 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.540676 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 函館 [MASK] [SEP] [MASK] 又は 交付 で 、 第 八 条 の 三 各 [MASK] に 掲げる 方法 に ##よ ##る 運用 の ため に する 金銭 等 の [MASK] [MASK] 交付 以外 の もの まっ いう 。 この 法律 の 規定 を 適用 ##する に ##ついて は 、 [MASK] 掲げる 団体 光沢 、 政治 団体 と みなす 。 政治 上 の 主義 又は [MASK] を 研究 連載 目的 [MASK] 有する 団体 で 、 衆議院 [MASK] 若しくは 参議院 議員 が 主宰 [MASK] もの 又は その [MASK] な 構成 員 が 衆議院 議員 若しくは 参議院 議員 である もの 政治 資金 団体 [MASK] [MASK] の ため に 資金 上 の 援助 を する 目的 を 有する 団体 で 、 [MASK] 六 条 の 二 第 二 項 前 [SEP]\n",
            "I0801 03:51:17.540835 139981399807872 create_pretraining_data.py:161] input_ids: 2 6923 4 3 4 4088 9251 13 6 55 1148 724 5 168 286 4 8 12560 771 8 4141 668 992 5 48 8 22 8676 174 5 4 4 9251 414 5 60 18665 56 7 46 1442 5 1333 10 2345 3244 8 19591 9 6 4 12560 714 18683 6 407 714 12 16036 7 407 100 5 408 4088 4 10 160 1111 548 4 1910 714 13 6 2023 4 17608 4388 539 11 5720 4 60 4088 58 4 276 427 563 11 2023 539 17608 4388 539 32 60 407 2096 714 4 4 5 48 8 2096 100 5 3813 10 22 548 10 1910 714 13 6 4 1346 724 5 129 55 129 1235 134 3\n",
            "I0801 03:51:17.540971 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.541104 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.541193 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 2 4 15 28 30 31 36 50 53 55 65 68 70 76 82 86 101 102 118 0\n",
            "I0801 03:51:17.541283 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 1081 14113 144 174 14113 4088 10 5297 9 407 13728 22 10 539 22 1681 16 2707 55 0\n",
            "I0801 03:51:17.541396 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.541493 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.542023 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.542230 139981399807872 create_pretraining_data.py:151] tokens: [CLS] さ れる [MASK] [MASK] なる 金額 の 全部 又は 一部 に ##つき 前 ##項 の 書類 の 添付 が [MASK] 確定 申告 書 の 提出 が あっ た 場合 に ##おい ##て も 、 その 書類 の 添付 が [MASK] ##っ た こと [MASK] ##ついて やむ ##を ##得 ##ない 事情 が ある と 認める とき は 、 その 書類 の 添付 が なか [SEP] ##務 （ 以下 「 物品 等 [MASK] と いう 。 ） の 調達 に ##当 ##た [MASK] は 、 障害 者 就労 施設 ＪＡＬ の 受注 の 機会 の 増大 を 図る ため 、 予算 の 適正 な 使用 に 留意 [MASK] [MASK] 、 優先 [MASK] に 障害 者 [MASK] [MASK] アル から 物品 [MASK] を 調達 する よう 努め な ##け [SEP]\n",
            "I0801 03:51:17.542406 139981399807872 create_pretraining_data.py:161] input_ids: 2 18 41 4 4 66 7097 5 7299 4088 224 8 7450 134 17126 5 9402 5 15816 11 4 4155 17141 520 5 2569 11 21400 1426 127 8 24314 2625 23 6 58 9402 5 15816 11 4 1588 1426 30 4 19591 19264 25246 10522 5342 3348 11 38 12 5703 372 9 6 58 9402 5 15816 11 2159 3 3938 16 200 24 16881 174 4 12 56 7 17 5 6230 8 9187 1920 4 9 6 1384 57 26417 482 26073 5 13390 5 2224 5 5541 10 4577 48 6 3460 5 20705 276 153 8 22626 4 4 6 3978 4 8 1384 57 4 4 641 27 16881 4 10 6230 22 762 21070 276 1874 3\n",
            "I0801 03:51:17.542554 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.542684 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.542771 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 3 4 20 34 37 40 44 71 81 88 98 106 107 110 111 114 115 116 119 0\n",
            "I0801 03:51:17.542862 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 30 12 47 6 5 2159 8 25 617 174 6 31 1321 147 8 26417 482 174 174 0\n",
            "I0801 03:51:17.542956 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.543035 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.543576 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.543791 139981399807872 create_pretraining_data.py:151] tokens: [CLS] [MASK] 附 ##則 第 二 条 の 規定 [MASK] 適用 に [MASK] 、 昭和 [MASK] 十 五 [MASK] 四 ##月 分 及び 同 ##年 五 ##月 分 [MASK] 掛 ##金 の うち 追加 し て [MASK] べき 掛 ##金 が ある とき は 、 給与 支給 [MASK] 若しくは 団体 等 （ 地方 公務 ##員 等 共済 組合 法 （ 以下 この 条 に ##おい ##て 「 法 」 [MASK] いう [MASK] ） 第 百 九 十 五 条 第 一 項 [MASK] ##規 [SEP] 名曲 政令 は 、 内閣 [MASK] の 一部 を 改正 ##する 法律 ヘレン 平成 [MASK] 一 年 法律 第 八 十 八 号 ） の 施行 [MASK] 三部作 （ 平成 [MASK] [MASK] 年 一 ##月 六 日 ） から 施行 する 。 [SEP]\n",
            "I0801 03:51:17.543946 139981399807872 create_pretraining_data.py:161] input_ids: 2 4 8842 6059 55 129 724 5 1333 4 2345 8 4 6 155 4 1677 684 4 535 2703 213 388 71 2373 684 2703 213 4 11271 2803 5 381 1175 31 3402 4 1450 11271 2803 11 38 372 9 6 11837 8113 4 17608 714 174 16 317 6762 5681 174 24761 1701 202 16 200 46 724 8 24314 2625 24 202 25 4 56 4 17 55 2659 2205 1677 684 724 55 70 1235 4 11884 3 28648 11692 9 6 1340 4 5 224 10 1577 3244 1442 22275 232 4 70 14 1442 55 1148 1677 1148 144 17 5 2287 4 17369 16 232 4 4 14 70 2703 1346 29 17 27 2287 22 7 3\n",
            "I0801 03:51:17.544081 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.544210 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.544298 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 1 9 12 15 18 28 36 47 69 71 82 85 90 97 99 111 112 115 116 0\n",
            "I0801 03:51:17.544403 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 17 5 17450 684 14 1 10591 386 12 7 8 46 202 16 1677 5 29 1677 168 0\n",
            "I0801 03:51:17.544515 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.544598 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.545109 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.545300 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 前 ##項 [MASK] [MASK] に ##より 構成 員 事業 主 が 拠出 し [MASK] 金銭 は 、 返還 を 受ける こと が でき ない 。 [SEP] 条 に ##規 ##定 ##する 特定 事業 を [MASK] [MASK] 以下 同じ 。 ） に ##ついて 同 条 の 規定 に ##より 国 が 通常 の 補助 の 幌 を 超え て [MASK] 年度 の 補助 を する こと と [MASK] 場合 に は [MASK] 農林 水産 大臣 は 、 当該 特定 [MASK] [UNK] [MASK] 超える 部分 の 額 [MASK] 当該 年度 の 翌 年度 に 交付 する ##もの ##と ##する 。 ただし 、 特別 の 理由 に ##より [MASK] [MASK] ##得 ##ない ファゴット が ある と 認め [MASK] 場合 に は 、 当該 年度 の 翌 変貌 年度 に [SEP]\n",
            "I0801 03:51:17.545964 139981399807872 create_pretraining_data.py:161] input_ids: 2 134 17126 4 4 8 17450 427 563 318 692 11 22931 31 4 8676 9 6 6613 10 1164 30 11 261 47 7 3 724 8 11884 3665 3244 1328 318 10 4 4 200 328 7 17 8 19591 71 724 5 1333 8 17450 85 11 586 5 3398 5 15616 10 10458 3402 4 594 5 3398 10 22 30 12 4 127 8 9 4 7990 7583 1025 9 6 4306 1328 4 1 4 2962 491 5 2822 4 4306 594 5 698 594 8 9251 22 10064 1878 3244 7 484 6 855 5 735 8 17450 4 4 10522 5342 24840 11 38 12 966 4 127 8 9 6 4306 594 5 698 17030 594 8 3\n",
            "I0801 03:51:17.546140 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.546271 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.546374 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 3 4 14 30 35 36 55 59 61 67 71 79 81 86 106 107 110 115 124 0\n",
            "I0801 03:51:17.546476 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 5 1333 1426 3665 56 7 5011 4306 5 66 6 318 58 10 19264 25246 3348 124 1515 0\n",
            "I0801 03:51:17.546575 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.546657 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.547145 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.547336 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 十 四 年 法律 第 百 五 [MASK] 一 号 ） 第 三 条 第 一 項 の 規定 に [MASK] 同 [MASK] に ##規 ##定 [MASK] 電子 情報 ##処 [MASK] 組織 を 使用 し て 行う 同 はん 第 二 条 第 六 号 に ##規 ##定 ##する 申請 等 [MASK] いう 。 以下 同じ 。 ） に ##よ ##る 場合 タイヤ あっ て は [MASK] 三 [MASK] 万 五 百 [MASK] ） に 、 その 申請 [UNK] 特殊 容器 （ 同 項 の 特殊 容器 を いう 。 以下 同じ 。 [MASK] の 製造 及び [SEP] 中華人民共和国 [MASK] [MASK] 地域 及び マカオ ##グナー を 除く 。 第 三 条 第 二 項 に ##おい ##て 「 特定 [MASK] 国 」 と いう 。 ） [SEP]\n",
            "I0801 03:51:17.547509 139981399807872 create_pretraining_data.py:161] input_ids: 2 1677 535 14 1442 55 2659 684 4 70 144 17 55 168 724 55 70 1235 5 1333 8 4 71 4 8 11884 3665 4 1424 402 14183 4 498 10 153 31 3402 305 71 9953 55 129 724 55 1346 144 8 11884 3665 3244 3930 174 4 56 7 200 328 7 17 8 4141 668 127 5961 21400 3402 9 4 168 4 2701 684 2659 4 17 8 6 58 3930 1 2827 9701 16 71 1235 5 2827 9701 10 56 7 200 328 7 4 5 487 388 3 3274 4 4 201 388 15093 23419 10 2262 7 55 168 724 55 129 1235 8 24314 2625 24 1328 4 85 25 12 56 7 17 3\n",
            "I0801 03:51:17.547651 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.547780 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.547867 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 8 17 21 23 27 31 39 49 52 63 67 69 73 94 99 100 101 105 120 0\n",
            "I0801 03:51:17.547957 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 1677 1235 17450 1235 3244 4404 202 3244 10 8 6 1677 387 17 3274 16 2774 201 10075 0\n",
            "I0801 03:51:17.548051 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.548128 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.548620 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.548796 139981399807872 create_pretraining_data.py:151] tokens: [CLS] [MASK] 百 記述 十 三 条 第 二 項 に ##規 [MASK] ##する 有価 ＧＳ の 売 ##付け ##ＴＣ を いう 。 以下 この 条 [MASK] [MASK] ##て 同じ 。 ） 又は 有価 証券 の 買 ##付け 等 （ 法 [MASK] 百 七 十 三 条 [MASK] 三 項 に ##規 ##定 ##する [MASK] 証券 [MASK] 買 ##付け 等 を いう 。 以下 この 条 に ##おい ##て 同じ 。 ） が 次 稀である 各 号 に 掲げる 取引 である とき は 、 [MASK] [MASK] 号 に [MASK] 取引 の 価格 は 、 当該 各 号 [SEP] 法 第 二 条 第 二 十 [MASK] [MASK] 第 二 号 に 掲げる 取引 [MASK] これ [MASK] 類似 する 外国 市場 デリバ ##ティブ 取引 を 含む 。 ） [SEP]\n",
            "I0801 03:51:17.548943 139981399807872 create_pretraining_data.py:161] input_ids: 2 4 2659 1105 1677 168 724 55 129 1235 8 11884 4 3244 27042 19652 5 9495 4545 14656 10 56 7 200 46 724 4 4 2625 328 7 17 4088 27042 4190 5 10715 4545 174 16 202 4 2659 1470 1677 168 724 4 168 1235 8 11884 3665 3244 4 4190 4 10715 4545 174 10 56 7 200 46 724 8 24314 2625 328 7 17 11 244 23773 286 144 8 12560 2246 32 372 9 6 4 4 144 8 4 2246 5 2125 9 6 4306 286 144 3 202 55 129 724 55 129 1677 4 4 55 129 144 8 12560 2246 4 73 4 3217 22 1283 1344 29881 8726 2246 10 609 7 17 3\n",
            "I0801 03:51:17.549072 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.549198 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.549284 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 1 3 12 15 19 26 27 41 47 54 56 74 84 85 88 105 106 113 115 0\n",
            "I0801 03:51:17.549390 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 55 1470 3665 4190 174 8 24314 55 55 27042 5 5 4306 286 12560 70 1235 16 8 0\n",
            "I0801 03:51:17.549495 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.549573 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0801 03:51:17.550046 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.550243 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 少 ##額 領 ##収 [MASK] 等 の 写し の 開示 は 、 閲覧 又は 写し の 交付 に [MASK] 行う 。 [SEP] 総務 大臣 又は 都道府県 の 選挙 管理 委員 会 は 、 [MASK] 五 項 の ##規 ##定 ##に ##よ ##る 命令 に 違反 し て 当該 国会 議員 関係 政治 [MASK] の 会計 責任 者 が 少 ##額 領 [MASK] 書 [MASK] の 写し を [MASK] し ない とき は 、 その 旨 [MASK] 開示 請求 [MASK] [MASK] 通知 する と ##とも ##に [MASK] その 旨 並びに [MASK] 国会 議員 関係 政治 団体 の [MASK] [MASK] 主たる 事務 ##所 の 所在 ##地 を 、 遅 ##滞 ##なく 、 インターネット の 利用 その他 [MASK] 適切 な 方法 に ##より [MASK] する ##もの ##と ##する 。 [SEP]\n",
            "I0801 03:51:17.550404 139981399807872 create_pretraining_data.py:161] input_ids: 2 5181 9498 772 13149 4 174 5 23003 5 21286 9 6 7205 4088 23003 5 9251 8 4 305 7 3 5907 1025 4088 5050 5 403 567 324 126 9 6 4 684 1235 5 11884 3665 361 4141 668 1814 8 4018 31 3402 4306 3120 539 250 407 4 5 5481 1824 57 11 5181 9498 772 4 520 4 5 23003 10 4 31 47 372 9 6 58 4578 4 21286 5111 4 4 10602 22 12 12670 361 4 58 4578 8729 4 3120 539 250 407 714 5 4 4 14261 739 2463 5 1414 1342 10 6 9808 25837 7748 6 2181 5 259 776 4 15966 276 771 8 17450 4 22 10064 1878 3244 7 3\n",
            "I0801 03:51:17.550546 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.550675 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.550763 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 2 5 19 34 47 53 62 64 68 76 79 80 86 90 94 97 98 115 121 0\n",
            "I0801 03:51:17.550851 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 9498 520 17450 55 3402 714 13149 174 2569 10 57 8 6 4306 407 545 388 422 3599 0\n",
            "I0801 03:51:17.550944 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.551021 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0801 03:51:17.551548 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.551727 139981399807872 create_pretraining_data.py:151] tokens: [CLS] マロ ##典 [MASK] ##ＡＳＨ 及び 伝達 の 実施 に [MASK] ##する こと 。 [SEP] ##隊 員 倫理 審査 会 の 庶 ##務 に ##関 ##する [MASK] 。 衛生 官 は 、 次に 掲げる 事務 を つか ##さ ##ど [MASK] 。 防衛 省 の 職員 の 保健 衛生 の 基本 [MASK] ##関 [MASK] こと 。 衛生 資材 の 調達 、 補給 及び 管理 の [MASK] に [MASK] ##する こと 。 衛生 資材 の 研究 開発 の 基本 に ##関 [MASK] こと 。 防衛 ##医 ##科 ##大 学校 [MASK] 管理 及び 運営 一般 に ##関 ##する [MASK] 。 地方 協力 局 に 、 次 の 八 課 並びに 沖縄 調整 官 一 [MASK] 及び 調達 官 一 人 [MASK] 置く [MASK] 地方 協力 企画 [MASK] 地方 調整 課 [SEP]\n",
            "I0801 03:51:17.551873 139981399807872 create_pretraining_data.py:161] input_ids: 2 23336 5794 4 21665 388 7447 5 570 8 4 3244 30 7 3 6351 563 8224 2626 126 5 18050 3938 8 6568 3244 4 7 5792 323 9 6 5297 12560 739 10 5352 868 3692 4 7 1320 335 5 3203 5 6688 5792 5 608 4 6568 4 30 7 5792 11591 5 6230 6 4946 388 567 5 4 8 4 3244 30 7 5792 11591 5 160 203 5 608 8 6568 4 30 7 1320 11626 4846 1448 172 4 567 388 681 309 8 6568 3244 4 7 317 1196 238 8 6 244 5 1148 3862 8729 1654 2723 323 70 4 388 6230 323 70 50 4 2011 4 317 1196 935 4 317 2723 3862 3\n",
            "I0801 03:51:17.552003 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.552131 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.552217 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 1 3 4 6 10 26 39 46 50 52 64 66 79 87 95 111 117 119 123 0\n",
            "I0801 03:51:17.552306 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 3952 5 3812 7447 6568 30 668 6688 8 3244 608 6568 3244 5 30 50 10 7 3862 0\n",
            "I0801 03:51:17.552425 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.552506 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0801 03:51:17.552984 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.553158 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 前 ##項 の 財務 ##省 令 で 定める 書類 の 添付 が ない 同 [MASK] の 申告 書 又は 更 ##正 [MASK] 書 の 提出 [MASK] あつ た 場合 [MASK] ##おい ##て も 、 その 添付 が なか ##つ た こと に ##ついて やむ ##を ##得 ##ない 事情 が [MASK] [MASK] 認める とき は 、 当該 [MASK] の 提出 が [MASK] た 場合 に 限り [SEP] 前 二 項 に 定める もの [MASK] [MASK] 、 [MASK] ##与 を し た 者 が 第 一 [MASK] に ##規 ##定 ##する 婚姻 [MASK] が 二 十 年 [MASK] である 配偶 者 に 該当 する か 否 か [MASK] 判定 その他 同 項 の 規定 の 適用 に [MASK] ##し 必要 な 事項 は 、 政令 [MASK] 定める 。 [SEP]\n",
            "I0801 03:51:17.553300 139981399807872 create_pretraining_data.py:161] input_ids: 2 134 17126 5 5540 11176 3208 13 6010 9402 5 15816 11 47 71 4 5 17141 520 4088 7134 2749 4 520 5 2569 4 12966 1426 127 4 24314 2625 23 6 58 15816 11 2159 2243 1426 30 8 19591 19264 25246 10522 5342 3348 11 4 4 5703 372 9 6 4306 4 5 2569 11 4 1426 127 8 1808 3 134 129 1235 8 6010 60 4 4 6 4 14124 10 31 1426 57 11 55 70 4 8 11884 3665 3244 10856 4 11 129 1677 14 4 32 16287 57 8 4072 22 90 9199 90 4 1914 776 71 1235 5 1333 5 2345 8 4 440 510 276 6109 9 6 11692 4 6010 7 3\n",
            "I0801 03:51:17.553450 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.553580 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.553667 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 15 22 25 26 30 40 50 51 57 61 73 74 76 85 91 96 106 116 124 0\n",
            "I0801 03:51:17.553756 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 1235 5111 2569 11 8 1426 38 12 9402 12966 5 353 12161 1235 689 269 5 6568 13 0\n",
            "I0801 03:51:17.553846 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.553923 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0801 03:51:17.554514 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.554711 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 障害 年金 差 ##額 [MASK] 時 金 [MASK] 、 遺族 [MASK] と みなし て 、 第 十 条 の 規定 を 適用 ##する 。 [SEP] ##る 評価 に ##関 ##する 庶 ##務 は 、 海上 クリスティーナ ##安 ##庁 警備 ##エッ 部 に ##おい ##て 処理 する [MASK] 十 [UNK] [UNK] [UNK] の 規定 に ##より 海上 災害 防止 [MASK] [MASK] 以下 [MASK] [MASK] センター 」 と いう 。 ） が 解散 し た とき は 、 国土 [MASK] 大臣 は 、 遅 ##滞 ##なく 、 [MASK] 解散 の 登記 [MASK] 登記 [MASK] に 嘱託 しな ##ければ ##なら ##ない 。 登記 官 は 、 前 ##項 の ##規 ##定 ##に ##よ ##る 嘱託 [UNK] 解散 の 登記 を し た とき は 、 その [MASK] 用紙 を [SEP]\n",
            "I0801 03:51:17.554856 139981399807872 create_pretraining_data.py:161] input_ids: 2 1384 7911 1271 9498 4 87 306 4 6 10592 4 12 25057 3402 6 55 1677 724 5 1333 10 2345 3244 7 3 668 656 8 6568 3244 18050 3938 9 6 2919 27577 2609 18805 3471 21298 110 8 24314 2625 1536 22 4 1677 1 1 1 5 1333 8 17450 2919 4172 3894 4 4 200 4 4 558 25 12 56 7 17 11 1777 31 1426 372 9 6 4337 4 1025 9 6 9808 25837 7748 6 4 1777 5 7045 4 7045 4 8 18914 20040 8919 11512 5342 7 7045 323 9 6 134 17126 5 11884 3665 361 4141 668 18914 1 1777 5 7045 10 31 1426 372 9 6 58 4 17082 10 3\n",
            "I0801 03:51:17.554988 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.555117 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.555203 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 5 8 11 30 36 40 42 47 59 60 62 63 76 78 86 90 92 98 124 0\n",
            "I0801 03:51:17.555293 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 70 9 13873 3244 3237 18201 8 7 558 16 24 299 6 664 58 10 131 5342 7045 0\n",
            "I0801 03:51:17.555400 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.555492 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.555947 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.556120 139981399807872 create_pretraining_data.py:151] tokens: [CLS] ただし 、 附 ##則 第 三 条 の 改正 規定 及び 附 ##則 第 [MASK] 条 から [MASK] 九 条 まで の 規定 [MASK] 、 公布 の 日 から 施行 する 。 [SEP] 、 公布 の 日 から 起 ##算 し て 九 ##月 [MASK] 超え ない [MASK] 内 に ##おい [MASK] [MASK] で 定める 日 （ [MASK] 「 施行 日 」 と いう 。 ） から 施行 する 。 この 法律 は 、 公布 の 日 から 起 [MASK] [MASK] て 二 十 ポン を 経過 し た [MASK] から 施行 する 。 この 法律 は 、 公布 の [MASK] から 施行 する 。 この 法律 は 、 公布 の 日 から 起 [MASK] し て 一 年 を [MASK] ない 範囲 内 に ##おい [SEP]\n",
            "I0801 03:51:17.556266 139981399807872 create_pretraining_data.py:161] input_ids: 2 484 6 8842 6059 55 168 724 5 1577 1333 388 8842 6059 55 4 724 27 4 2205 724 63 5 1333 4 6 10529 5 29 27 2287 22 7 3 6 10529 5 29 27 5288 6788 31 3402 2205 2703 4 10458 47 4 76 8 24314 4 4 13 6010 29 16 4 24 2287 29 25 12 56 7 17 27 2287 22 7 46 1442 9 6 10529 5 29 27 5288 4 4 3402 129 1677 4900 10 4756 31 1426 4 27 2287 22 7 46 1442 9 6 10529 5 4 27 2287 22 7 46 1442 9 6 10529 5 29 27 5288 4 31 3402 70 14 10 4 47 2039 76 8 24314 3\n",
            "I0801 03:51:17.556412 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.556553 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.556639 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 3 8 10 15 18 24 43 45 48 52 53 58 80 81 85 90 101 115 121 0\n",
            "I0801 03:51:17.556728 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 8842 5 1333 1470 55 9 2205 10 2039 2625 11692 200 6788 31 29 29 29 6788 10458 0\n",
            "I0801 03:51:17.556818 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.556895 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0801 03:51:17.557385 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.557571 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 組合 は 、 その 行う 事業 に よ つ ##て その [MASK] 員 に 直接 の 奉仕 を する こと を ##史 [MASK] し 、 特定 の 組合 員 の 利益 のみ を 目的 とし ##て [MASK] 事業 を 行 つ [MASK] は なら ない 。 [SEP] 営業 線 の 名称 及び 区間 を 明らか パルプ する こと 。 連絡 船 事業 航路 の 名称 及び 区間 を 明らか [MASK] する こと 。 旅客 自動車 運送 事業 路線 の 名称 及び [MASK] 又は 事業 区域 を [MASK] に する こと 。 法 円墳 十 一 条 第 一 項 に ##規 ##定 ##する [MASK] [MASK] [MASK] ##関 ##する 業務 [SEP]\n",
            "I0801 03:51:17.557719 139981399807872 create_pretraining_data.py:161] input_ids: 2 1701 9 6 58 305 318 8 1291 83 2625 58 4 563 8 1331 5 12343 10 22 30 10 3168 4 31 6 1328 5 1701 563 5 3718 190 10 548 19784 2625 4 318 10 1512 83 4 9 371 47 7 3 911 120 5 545 388 949 10 10092 23390 22 30 7 1956 490 318 5138 5 545 388 949 10 10092 4 22 30 7 2158 743 10386 318 456 5 545 388 4 4088 318 2494 10 4 8 22 30 7 202 14147 1677 70 724 55 70 1235 8 11884 3665 3244 4 4 4 6568 3244 1343 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 03:51:17.557847 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 03:51:17.557976 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 03:51:17.558065 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 12 15 22 23 37 42 55 56 70 73 82 87 89 93 104 105 106 0 0 0\n",
            "I0801 03:51:17.558154 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 1701 1331 548 12 58 2625 10092 8 8 7 949 10092 22 55 1026 1022 8 0 0 0\n",
            "I0801 03:51:17.558245 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0801 03:51:17.558322 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.558813 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.558990 139981399807872 create_pretraining_data.py:151] tokens: [CLS] に ##おい ##て [MASK] ##則 第 二 十 九 条 の 七 第 一 項 の 規定 [MASK] 適用 を 受け ない こと と なつ た 場合 に [MASK] [MASK] 当該 年度 を いう 。 以下 この 条 に ##おい ##て [MASK] 。 [MASK] 分 市街 ##化 ##区 ##域 ＄ 年度 の 翌 年度 [MASK] [MASK] 市 と [MASK] た 年度 の 翌 [SEP] ある 地方 自治 [MASK] 第 二 百 五 十 二 条 の [MASK] 九 第 一 項 の 市 の 区域 又は その他 ##の 市 で その 区域 の 全部 若しくは 一部 [MASK] 首都 圏 整備 法 第 二 条 第 三 [MASK] に ##規 ##定 ##する 既成 市街 ##地 若しくは 同 条 第 野 項 に ##規 ##定 ##する 近郊 [MASK] [SEP]\n",
            "I0801 03:51:17.559134 139981399807872 create_pretraining_data.py:161] input_ids: 2 8 24314 2625 4 6059 55 129 1677 2205 724 5 1470 55 70 1235 5 1333 4 2345 10 441 47 30 12 25352 1426 127 8 4 4 4306 594 10 56 7 200 46 724 8 24314 2625 4 7 4 213 2171 3453 8062 6719 14477 594 5 698 594 4 4 65 12 4 1426 594 5 698 3 38 317 825 4 55 129 2659 684 1677 129 724 5 4 2205 55 70 1235 5 65 5 2494 4088 776 422 65 13 58 2494 5 7299 17608 224 4 1679 934 887 202 55 129 724 55 168 4 8 11884 3665 3244 27834 2171 1342 17608 71 724 55 1981 1235 8 11884 3665 3244 4023 4 3\n",
            "I0801 03:51:17.559263 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.559406 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.559504 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 1 4 18 29 30 42 44 50 55 56 59 68 77 94 97 107 119 125 126 0\n",
            "I0801 03:51:17.559593 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 8 8842 5 2920 3316 328 17 623 213 1328 25352 202 1677 7299 11 1235 535 4023 887 0\n",
            "I0801 03:51:17.559686 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.559763 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0801 03:51:17.560249 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.560454 139981399807872 create_pretraining_data.py:151] tokens: [CLS] この 法律 は 、 公布 の 日 から 起 ##算 し て 十 ##月 を [MASK] ##え ない 範囲 内 に ##おい ##て [MASK] 各 規定 に つき [MASK] 政令 で [MASK] [MASK] から 施行 する 。 [SEP] 。 債券 の 発行 の 年月日 債券 の 数 （ [MASK] ##債 [MASK] 振替 法 の [MASK] の 適用 が ない とき [MASK] 、 債券 の 数 及び 番号 ） [MASK] 十 一 条 第 三 項 第 集まる 号 から 第 六 号 [MASK] 、 第 八 超過 及び 第 [MASK] 一 号 に [MASK] 事項 元 ##利 金 の 支払 に ##関 ##する 事項 日本 学生 支援 債券 を 償還 する [MASK] に ##おい ##て 、 欠 ##け て いる 利 [MASK] が ある とき は [SEP]\n",
            "I0801 03:51:17.560605 139981399807872 create_pretraining_data.py:161] input_ids: 2 46 1442 9 6 10529 5 29 27 5288 6788 31 3402 1677 2703 10 4 1247 47 2039 76 8 24314 2625 4 286 1333 8 3050 4 11692 13 4 4 27 2287 22 7 3 7 24254 5 945 5 27773 24254 5 145 16 4 21196 4 27908 202 5 4 5 2345 11 47 372 4 6 24254 5 145 388 984 17 4 1677 70 724 55 168 1235 55 9478 144 27 55 1346 144 4 6 55 1148 14626 388 55 4 70 144 8 4 6109 164 3379 306 5 14802 8 6568 3244 6109 49 1098 809 24254 10 31332 22 4 8 24314 2625 6 9121 1874 3402 21 2637 4 11 38 372 9 3\n",
            "I0801 03:51:17.560736 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.560864 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.560950 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 8 16 24 29 32 33 39 49 51 55 61 69 77 83 87 90 94 112 122 0\n",
            "I0801 03:51:17.561041 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 27 1259 6 6 6010 29 7 173 174 1333 9 55 70 63 144 1677 12560 127 8538 0\n",
            "I0801 03:51:17.561132 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.561207 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.561698 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.561875 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 後見 監督 人 の 選任 の 大まかに [MASK] （ 別表 [MASK] 一 の 六 の 項 の 事項 に ##ついて の 審判 事件 を いう 。 ） [MASK] 後見 監督 人 [MASK] 解任 の 審判 事件 （ 別表 第 一 の [MASK] の 項 [MASK] 事項 に ##ついて の 審判 事件 を いう 。 第 百 二 十 七 条 第 五 項 [SEP] 法 に [MASK] ##る 年金 である 給付 に [MASK] は 、 [MASK] [MASK] [MASK] 一 年 の 年 平均 の 物価 指数 に [MASK] ##する 昭和 六 十 二 年 の 年 平均 [MASK] 物価 指数 の 比率 を 基準 とし ##て [MASK] [MASK] 六 十 三 年 四 [MASK] 分 以後 の 当該 年金 である [MASK] の 額 [MASK] 改定 する [SEP]\n",
            "I0801 03:51:17.562023 139981399807872 create_pretraining_data.py:161] input_ids: 2 20838 290 50 5 15669 5 26322 4 16 23059 4 70 5 1346 5 1235 5 6109 8 19591 5 5883 390 10 56 7 17 4 20838 290 50 4 6625 5 5883 390 16 23059 55 70 5 4 5 1235 4 6109 8 19591 5 5883 390 10 56 7 55 2659 129 1677 1470 724 55 684 1235 3 202 8 4 668 7911 32 13873 8 4 9 6 4 4 4 70 14 5 14 1059 5 15819 9161 8 4 3244 155 1346 1677 129 14 5 14 1059 4 15819 9161 5 6358 10 1726 19784 2625 4 4 1346 1677 168 14 535 4 213 936 5 4306 7911 32 4 5 2822 4 10346 22 3\n",
            "I0801 03:51:17.562154 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.562308 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.562411 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 7 8 11 28 32 42 45 67 73 76 77 78 88 98 107 108 114 121 124 0\n",
            "I0801 03:51:17.562513 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 5883 390 55 10651 5 1148 5 4141 19591 155 1346 1677 15994 5 6 155 2703 13873 10 0\n",
            "I0801 03:51:17.562604 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.562681 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.563151 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.563327 139981399807872 create_pretraining_data.py:151] tokens: [CLS] [MASK] 主 は 、 労働 者 が 附 ##則 第 二 条 第 一 項 の ##規 ##定 ##に [MASK] [MASK] 保険 関係 の 成立 [MASK] [MASK] し [MASK] こと を 理由 とし [MASK] 、 労働 者 に ##対 ##して 解雇 その他 不利 ##益 な 取扱い を して ##は ##なら ##ない 。 [SEP] 規定 に [MASK] し た とき は 、 六 箇 ##月 以下 [MASK] 懲役 又は 三 十 万 円 以下 の 罰金 に [MASK] ##する 。 法人 の 代表 者 又は 法人 若しくは 人 [MASK] 代理人 、 使用人 その他 ##の [MASK] 者 が 、 [MASK] 法人 又は 人 の 業務 に [MASK] ##して 、 [MASK] ##項 の [MASK] 行為 を し [MASK] とき は 、 [MASK] 者 を 罰 ##する ほか 、 その [SEP]\n",
            "I0801 03:51:17.563494 139981399807872 create_pretraining_data.py:161] input_ids: 2 4 692 9 6 759 57 11 8842 6059 55 129 724 55 70 1235 5 11884 3665 361 4 4 2314 250 5 986 4 4 31 4 30 10 735 19784 4 6 759 57 8 15994 1037 5923 776 16545 8103 276 22321 10 19 7019 11512 5342 7 3 1333 8 4 31 1426 372 9 6 1346 18455 2703 200 4 8393 4088 168 1677 2701 387 200 5 12165 8 4 3244 7 907 5 191 57 4088 907 17608 50 4 13537 6 25483 776 422 4 57 11 6 4 907 4088 50 5 1343 8 4 1037 6 4 17126 5 4 1381 10 31 4 372 9 6 4 57 10 11152 3244 353 6 58 3\n",
            "I0801 03:51:17.563624 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.563751 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.563836 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 1 20 21 26 27 29 34 38 56 66 77 88 94 98 105 108 111 115 119 0\n",
            "I0801 03:51:17.563925 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 318 4141 668 10 3026 1426 2625 8 4018 5 9859 5 5865 58 6568 134 4018 1426 1381 0\n",
            "I0801 03:51:17.564046 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.564127 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0801 03:51:17.564633 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.564813 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 措置 法 第 六 十 八 条 の 七 十 八 第 五 項 及び 第 六 項 の [MASK] は 第 一 項 又は 第 八 項 の 規定 を 適用 ##する 場合 に ##ついて 、 同 アフ 第 [MASK] 項 及び 第 八 項 [MASK] 規定 は 第 [MASK] 項 又は 第 九 項 の 規定 の [MASK] を 受け た 買 換 ##資 産 に ##ついて [MASK] 同 条 第 十 一 項 の [MASK] は 第 九 項 [MASK] 規定 を 適用 ##する 場合 [SEP] [MASK] 法律 は 、 公布 の 日 から 施行 する 。 この 法律 取り出す 、 平成 十 九 年 [MASK] ##月 一 日 から [MASK] し 、 平成 十 九 [MASK] [MASK] 予算 [MASK] 適用 ##する 。 [SEP]\n",
            "I0801 03:51:17.564957 139981399807872 create_pretraining_data.py:161] input_ids: 2 3854 202 55 1346 1677 1148 724 5 1470 1677 1148 55 684 1235 388 55 1346 1235 5 4 9 55 70 1235 4088 55 1148 1235 5 1333 10 2345 3244 127 8 19591 6 71 8827 55 4 1235 388 55 1148 1235 4 1333 9 55 4 1235 4088 55 2205 1235 5 1333 5 4 10 441 1426 10715 12187 11828 2612 8 19591 4 71 724 55 1677 70 1235 5 4 9 55 2205 1235 4 1333 10 2345 3244 127 3 4 1442 9 6 10529 5 29 27 2287 22 7 46 1442 25615 6 232 1677 2205 14 4 2703 70 29 27 4 31 6 232 1677 2205 4 4 3460 4 2345 3244 7 3\n",
            "I0801 03:51:17.565087 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.565213 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.565298 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 20 38 39 41 47 51 60 70 78 83 87 90 94 103 109 114 120 121 123 0\n",
            "I0801 03:51:17.565403 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 1333 71 724 1470 5 1148 2345 6 1333 5 3244 46 10529 9 535 2287 594 5 27 0\n",
            "I0801 03:51:17.565522 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.565596 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.566070 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.566241 139981399807872 create_pretraining_data.py:151] tokens: [CLS] 法 附 ##則 第 二 条 の [MASK] に ##より なお [MASK] ##前 [MASK] [MASK] ##に ##よ ##る こと と さ れ た 旧 地方 税法 附 ##則 第 九 条 の 六 第 三 項 前 ##段 」 と 、 「 同 項 後 ##段 」 と [MASK] の [MASK] 「 附 ##則 第 九 条 の 六 第 くじ 項 後 ヨーガ 及び 地方 税法 等 改正 法 附 ##則 第 二 条 の [MASK] に [MASK] なお ##従 ##前 ##の [MASK] ##に ##よ ##る こと と さ れ [MASK] 旧 地方 税法 附 [MASK] 第 [SEP] 専門 委員 [MASK] [MASK] 当該 専門 の 事項 に ##関 ##し 学識 経験 を 有する 者 [MASK] うち から 、 内閣 総理 大臣 が 任命 ディラン 。 [SEP]\n",
            "I0801 03:51:17.566400 139981399807872 create_pretraining_data.py:161] input_ids: 2 202 8842 6059 55 129 724 5 4 8 17450 194 4 2676 4 4 361 4141 668 30 12 18 54 1426 299 317 25159 8842 6059 55 2205 724 5 1346 55 168 1235 134 6896 25 12 6 24 71 1235 62 6896 25 12 4 5 4 24 8842 6059 55 2205 724 5 1346 55 14766 1235 62 26004 388 317 25159 174 1577 202 8842 6059 55 129 724 5 4 8 4 194 13147 2676 422 4 361 4141 668 30 12 18 54 4 299 317 25159 8842 4 55 3 673 324 4 4 4306 673 5 6109 8 6568 440 30506 1004 10 1910 57 4 381 27 6 1340 5538 1025 11 1923 20285 7 3\n",
            "I0801 03:51:17.566544 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.566669 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.566768 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 8 12 14 15 36 49 51 61 64 77 79 82 84 92 97 102 103 116 125 0\n",
            "I0801 03:51:17.566859 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 1333 13147 422 11044 1235 38 9 168 6896 1333 17450 2676 11044 1426 6059 9 6 5 22 0\n",
            "I0801 03:51:17.566952 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.567029 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0801 03:51:17.567517 139981399807872 create_pretraining_data.py:149] *** Example ***\n",
            "I0801 03:51:17.567694 139981399807872 create_pretraining_data.py:151] tokens: [CLS] [MASK] 、 [MASK] の 各 号 に Ｈａｌ 規定 は 、 当該 各 号 に 定める 日 から 施行 高層 。 [MASK] 附 宇治 [MASK] 二 十 六 条 から 第 六 十 条 [MASK] [MASK] 第 六 十 二 条 から 第 [MASK] 十 [MASK] 条 まで [MASK] 規定 [SEP] 措置 法 、 エネルギー 等 の 使用 の 合理 [MASK] 及び 資源 の [MASK] な 利用 に ##関 [MASK] 事業 活動 [MASK] 促進 に ##関 ##する 臨時 措置 法 、 民間 資金 等 の 活用 に ##よ ##る 公共 施設 等 の 整備 等 の 促進 に ##関 [MASK] 法律 ##０９ ##の 法律 （ [MASK] に 基づく 命令 を 含む 。 ） の 規定 に ##より 政 投 銀 の 投 ##融 ##資 機能 が [SEP]\n",
            "I0801 03:51:17.567841 139981399807872 create_pretraining_data.py:161] input_ids: 2 4 6 4 5 286 144 8 30384 1333 9 6 4306 286 144 8 6010 29 27 2287 7979 7 4 8842 9548 4 129 1677 1346 724 27 55 1346 1677 724 4 4 55 1346 1677 129 724 27 55 4 1677 4 724 63 4 1333 3 3854 202 6 1660 174 5 153 5 8155 4 388 3678 5 4 276 259 8 6568 4 318 169 4 4688 8 6568 3244 3078 3854 202 6 2228 2096 174 5 3368 8 4141 668 2359 482 174 5 887 174 5 4688 8 6568 4 1442 9211 422 1442 16 4 8 2873 1814 10 609 7 17 5 1333 8 17450 1787 5175 1804 5 5175 22702 11828 557 11 3\n",
            "I0801 03:51:17.567973 139981399807872 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.568103 139981399807872 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0801 03:51:17.568191 139981399807872 create_pretraining_data.py:161] masked_lm_positions: 1 3 8 20 22 24 25 35 36 44 46 49 61 65 70 73 100 102 106 0\n",
            "I0801 03:51:17.568281 139981399807872 create_pretraining_data.py:161] masked_lm_ids: 484 244 12560 22 4656 6059 55 63 388 1346 684 5 102 4165 3244 5 3244 776 1442 0\n",
            "I0801 03:51:17.568388 139981399807872 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0801 03:51:17.568480 139981399807872 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0801 03:53:53.288208 139981399807872 create_pretraining_data.py:166] Wrote 467383 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUvehyMJNL_Z",
        "colab_type": "text"
      },
      "source": [
        "#### pretraining関係の定数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqWsLb6vMDfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = \"{}/model\".format(DATASET_PATH)\n",
        "\n",
        "JPNBERT_RELPATH = \"bert/Japanese_L-12_H-768_A-12_E-30_BPE\" # @param {\"type\": \"string\"}\n",
        "JPNBERT_PATH = \"gs://{}/{}\".format(BUCKET, JPNBERT_RELPATH)\n",
        "\n",
        "BERT_CONFIG_FILE = \"{}/bert_config.json\".format(JPNBERT_PATH)\n",
        "INIT_CP = \"{}/bert_model.ckpt\".format(JPNBERT_PATH)\n",
        "\n",
        "USE_TPU = True\n",
        "\n",
        "SAVE_CHECKPOINTS_STEPS = 20000\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_STEPS = 100000\n",
        "NUM_WARMUP_STEPS = 1000\n",
        "NUM_EVAL_STEPS = 100\n",
        "\n",
        "TRAIN_BATCH_SIZE = 32 #512\n",
        "EVAL_BATCH_SIZE = 512 #8\n",
        "PRED_BATCH_SIZE = 64 #8\n",
        "\n",
        "# params that are defined in config file\n",
        "MAX_SEQ_LENGTH = 128\n",
        "MAX_PREDICTIONS_PER_SEQ = 20\n",
        "\n",
        "\n",
        "import run_pretraining\n",
        "import modeling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU5Cp1E9MyFn",
        "colab_type": "text"
      },
      "source": [
        "### finetuning\n",
        "\n",
        "parameters are inherited from 日本語BERT and original bert\n",
        "https://github.com/google-research/bert#pre-training-with-bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dk-rRUV6eht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_estimator(init_checkpoint=INIT_CP, model_dir=MODEL_PATH):\n",
        "  bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG_FILE)\n",
        "  \n",
        "  tpu_cluster_resolver = None\n",
        "  if USE_TPU and TPU_ADDRESS:\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=model_dir,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      keep_checkpoint_max=10,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=is_per_host))\n",
        "\n",
        "  model_fn = run_pretraining.model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=init_checkpoint,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=NUM_TRAIN_STEPS,\n",
        "      num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=USE_TPU)\n",
        "  \n",
        "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "  # or GPU.\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=USE_TPU,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=TRAIN_BATCH_SIZE,\n",
        "      eval_batch_size=EVAL_BATCH_SIZE,\n",
        "      predict_batch_size=PRED_BATCH_SIZE)\n",
        "  \n",
        "  return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbyrK56YHdjw",
        "colab_type": "code",
        "outputId": "8508ffdd-6c18-4bd4-96d3-25a4768819c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# exec this if you use TPU\n",
        "if USE_TPU:\n",
        "  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    print('TPU devices:')\n",
        "    pprint.pprint(session.list_devices())\n",
        "\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.60.154.58:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 4580005805586410807),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17394964189653461345),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2257332226448154062),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4768680384725283491),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 18253737298473624293),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10979287439995388824),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3790691428889049938),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7594210683778320102),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 4874217104154499495),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4682418260358879486),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 10519127009647868906)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7TOVxtfRwFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = None\n",
        "def pretrain(do_train=True, do_eval=True):\n",
        "  global estimator\n",
        "\n",
        "  tf.gfile.MakeDirs(MODEL_PATH)\n",
        "\n",
        "  input_files = []\n",
        "  for input_pattern in TFRECORD_FILE.split(\",\"):\n",
        "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
        "\n",
        "  tf.logging.info(\"*** Input Files ***\")\n",
        "  for input_file in input_files:\n",
        "    tf.logging.info(\"  %s\" % input_file)\n",
        "\n",
        "  estimator = make_estimator()\n",
        "\n",
        "  if do_train:\n",
        "    tf.logging.info(\"***** Running training *****\")\n",
        "    tf.logging.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
        "    train_input_fn = run_pretraining.input_fn_builder(\n",
        "        input_files=input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS_PER_SEQ,\n",
        "        is_training=True)\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=NUM_TRAIN_STEPS)\n",
        "\n",
        "  if do_eval:\n",
        "    tf.logging.info(\"***** Running evaluation *****\")\n",
        "    tf.logging.info(\"  Batch size = %d\", EVAL_BATCH_SIZE)\n",
        "\n",
        "    eval_input_fn = run_pretraining.input_fn_builder(\n",
        "        input_files=input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS_PER_SEQ,\n",
        "        is_training=False)\n",
        "\n",
        "    result = estimator.evaluate(\n",
        "        input_fn=eval_input_fn, steps=NUM_EVAL_STEPS)\n",
        "\n",
        "    output_eval_file = os.path.join(MODEL_PATH, \"eval_results.txt\")\n",
        "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "      tf.logging.info(\"***** Eval results *****\")\n",
        "      for key in sorted(result.keys()):\n",
        "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9QiSvjcP0uj",
        "colab_type": "code",
        "outputId": "c626839c-c0d2-4a15-f6c3-f1beed13a893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.WARN)\n",
        "pretrain(do_train=True, do_eval=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0801 03:58:33.414719 139677679544192 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f08f46c2b70>) includes params argument, but params are not passed to Estimator.\n",
            "W0801 03:58:33.852328 139677679544192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0801 03:58:33.876772 139677679544192 deprecation_wrapper.py:119] From bert-tltc/run_pretraining.py:347: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0801 03:58:33.889818 139677679544192 deprecation.py:323] From bert-tltc/run_pretraining.py:378: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0801 03:58:33.891431 139677679544192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0801 03:58:33.936151 139677679544192 deprecation.py:323] From bert-tltc/run_pretraining.py:398: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0801 03:58:33.937553 139677679544192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0801 03:58:33.946519 139677679544192 deprecation_wrapper.py:119] From bert-tltc/run_pretraining.py:406: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0801 03:58:33.960134 139677679544192 deprecation.py:323] From bert-tltc/run_pretraining.py:413: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0801 03:58:34.059486 139677679544192 deprecation_wrapper.py:119] From bert-tltc/run_pretraining.py:117: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0801 03:58:34.061543 139677679544192 deprecation_wrapper.py:119] From bert-tltc/modeling.py:181: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0801 03:58:34.068494 139677679544192 deprecation_wrapper.py:119] From bert-tltc/modeling.py:459: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0801 03:58:34.117566 139677679544192 deprecation_wrapper.py:119] From bert-tltc/modeling.py:543: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0801 03:58:34.186544 139677679544192 deprecation.py:506] From bert-tltc/modeling.py:407: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0801 03:58:34.214071 139677679544192 deprecation.py:323] From bert-tltc/modeling.py:727: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0801 03:58:40.558310 139677679544192 deprecation_wrapper.py:119] From bert-tltc/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0801 03:58:40.562307 139677679544192 deprecation_wrapper.py:119] From bert-tltc/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0801 03:58:40.585550 139677679544192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0801 03:58:41.233832 139677679544192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0801 03:58:56.473300 139677679544192 deprecation_wrapper.py:119] From bert-tltc/run_pretraining.py:160: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "W0801 03:58:58.489986 139677679544192 deprecation_wrapper.py:119] From bert-tltc/run_pretraining.py:161: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
            "\n",
            "W0801 04:00:18.261146 139677679544192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEkEchgFYosz",
        "colab_type": "text"
      },
      "source": [
        "### prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESRlq1J5wqy_",
        "colab_type": "code",
        "outputId": "73b04848-c1d2-4ab8-e465-aeac99a2b35c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "CLIST_FILE = \"{}/common_format5_statutory_27_unite.json\".format(DATASET_PATH)\n",
        "TFRECORD_PRED_FILE = \"{}/tf_examples_pred.tfrecord\".format(DATASET_PATH)\n",
        "ANSWER_FILE = \"{}/answers.txt\".format(DATASET_PATH)\n",
        "\n",
        "# load dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "ADDR_DATASET_BASE = \"/content/gdrive/My Drive/colab_data/\"\n",
        "NAME_DATASET =  'bert_unite_global_train-eval-test_jpnbert/{}' #@param {type:\"string\"}\n",
        "TASK_DATA_DIR = ADDR_DATASET_BASE + NAME_DATASET\n",
        "TASK_DATA_FILE = ADDR_DATASET_BASE + NAME_DATASET + \"/{}.tsv\"\n",
        "\n",
        "def get_name_resultfile(prefix):\n",
        "  return DATASET_PATH + \"/result_{}\".format(prefix)\n",
        "\n",
        "def set_name_dataset(dataset):\n",
        "  global NAME_DATASET, TASK_DATA_DIR\n",
        "  NAME_DATASET = dataset\n",
        "  TASK_DATA_DIR = ADDR_DATASET_BASE + NAME_DATASET\n",
        "set_name_dataset(NAME_DATASET)\n",
        "\n",
        "def get_addr_dataset(termsetid, datatype=\"train\"):\n",
        "  if datatype == \"test\":\n",
        "    num = 0\n",
        "  elif datatype == \"train\":\n",
        "    num = 1\n",
        "  elif datatype == \"eval\":\n",
        "    num = 2\n",
        "  else:\n",
        "    num = 0\n",
        "  \n",
        "  ret = TASK_DATA_FILE.format(termsetid, num)\n",
        "  return ret\n",
        "\n",
        "def get_addr_datasetdir(termsetid):\n",
        "  ret = TASK_DATA_DIR.format(termsetid)\n",
        "  return ret\n",
        "\n",
        "\n",
        "# use to delete tf.flags to avoid collision\n",
        "def delete_tfflags(keys):\n",
        "  for key in keys:\n",
        "    if hasattr(tf.flags.FLAGS, key):\n",
        "      tf.flags.FLAGS.__delattr__(key)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z-9Pj8Es1qm",
        "colab_type": "code",
        "outputId": "dbc8899d-556b-4b7d-90d0-580735387740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import json\n",
        "\n",
        "import tokenization\n",
        "\n",
        "class ChoiceMaker:\n",
        "  \"\"\"\n",
        "  Offer function f: gtid -> (wid_answer, [wids_sametermset])\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, vocab_file=VOCAB_FILE, clist_file=CLIST_FILE):\n",
        "    self.WID_NONE = -1\n",
        "    self.vocab = tokenization.load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.clist = ChoiceMaker.load_clist(clist_file)\n",
        "    \n",
        "    self.gtid2term = []\n",
        "    self.gtid2wid = []\n",
        "    self.gtid2lid = []\n",
        "    self.lid2wids = []\n",
        "    self.lid2gtids = []\n",
        "    \n",
        "    for i, terms in enumerate(self.clist):\n",
        "      self.lid2wids += [[]]\n",
        "      self.lid2gtids += [[]]\n",
        "      \n",
        "      for term in terms[\"terms\"]:\n",
        "        term_unite = \" \".join(term)\n",
        "        \n",
        "        self.gtid2term += [term_unite]\n",
        "        self.gtid2lid += [i]\n",
        "        \n",
        "        wid = self.get_wid(term_unite)\n",
        "        \n",
        "        self.gtid2wid += [wid]\n",
        "        \n",
        "        if wid != self.WID_NONE:\n",
        "          self.lid2wids[-1] += [wid]\n",
        "          self.lid2gtids[-1] += [len(self.gtid2term)-1]\n",
        "  \n",
        "  @classmethod\n",
        "  def load_clist(cls, path):\n",
        "    with tf.gfile.GFile(path, \"r\") as f:\n",
        "      clist_tmp = json.load(f)[\"0\"]\n",
        "\n",
        "    clist = []\n",
        "    terms_str_exist = set()\n",
        "    for term_from in clist_tmp:\n",
        "        terms_str = \"__and__\".join(list(sorted([\" \".join(term) for term in clist_tmp[term_from][\"terms\"]])))\n",
        "        if terms_str not in terms_str_exist:\n",
        "            clist += [{\"terms\": terms_str, \"comment\": clist_tmp[term_from][\"comment\"]}]\n",
        "            terms_str_exist.add(terms_str)\n",
        "    for terms in clist:\n",
        "        terms_str = terms[\"terms\"]\n",
        "        terms[\"terms\"] = [term.split(\" \") for term in terms_str.split(\"__and__\")]\n",
        "\n",
        "    return clist\n",
        "  \n",
        "  def get_wid(self, term):\n",
        "    if term in self.vocab:\n",
        "      return self.vocab[term]\n",
        "    else:\n",
        "      return self.WID_NONE\n",
        "  \n",
        "  def get_choices_from_gtid(self, gtid):\n",
        "    lid = self.gtid2lid[gtid]\n",
        "    return self.lid2wids[lid]\n",
        "\n",
        "cm = ChoiceMaker()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 09:48:38.520940 139768208836480 deprecation_wrapper.py:119] From bert-tltc/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXXVzwSk8Ifk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "delete_tfflags([\n",
        "    \"vocab_file\", \"output_dir\", \"init_checkpoint\", \"max_seq_length\", \"do_train\", \"do_eval\", \n",
        "    \"train_batch_size\", \"eval_batch_size\", \"learning_rate\", \"save_checkpoints_steps\",\n",
        "    \"iterations_per_loop\", \"use_tpu\", \"tpu_name\", \"tpu_zone\", \"gcp_project\", \n",
        "    \"num_tpu_cores\", \"input_file\", \"output_dir\", \"max_predictions_per_seq\"\n",
        "])\n",
        "from create_pretraining_data import create_int_feature, create_float_feature\n",
        "\n",
        "answers = []\n",
        "localqid2globalqid = []\n",
        "def write_masklm_tfrecord_from_classifier_features(features, dst=TFRECORD_PRED_FILE):\n",
        "  writer = tf.python_io.TFRecordWriter(dst)\n",
        "    \n",
        "  for i, feature in enumerate(features): # feature is an instance of InputFeatures\n",
        "    # 必要なフィールドを追加\n",
        "    gtid = int(feature.label_id)\n",
        "    wid_answer = cm.gtid2wid[gtid]\n",
        "    if wid_answer == cm.WID_NONE:\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      feature.masked_lm_positions = [feature.input_ids.index(cm.vocab[\"[MASK]\"])]\n",
        "    except ValueError:\n",
        "      continue\n",
        "    \n",
        "    feature.masked_lm_ids = [wid_answer]\n",
        "    feature.masked_lm_weights = [1.0]\n",
        "\n",
        "    while len(feature.masked_lm_positions) < MAX_PREDICTIONS_PER_SEQ:\n",
        "      feature.masked_lm_positions.append(0)\n",
        "      feature.masked_lm_ids.append(0)\n",
        "      feature.masked_lm_weights.append(0.0)\n",
        "\n",
        "\n",
        "    # tfrecordに変換\n",
        "    feature_tfr = OrderedDict()\n",
        "    feature_tfr[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "    feature_tfr[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "    feature_tfr[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "    feature_tfr[\"masked_lm_positions\"] = create_int_feature(feature.masked_lm_positions)\n",
        "    feature_tfr[\"masked_lm_ids\"] = create_int_feature(feature.masked_lm_ids)\n",
        "    feature_tfr[\"masked_lm_weights\"] = create_float_feature(feature.masked_lm_weights)\n",
        "    feature_tfr[\"next_sentence_labels\"] = create_int_feature([0])\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature_tfr))\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "    \n",
        "    answers.append(gtid)\n",
        "    localqid2globalqid.append(i)\n",
        "  \n",
        "  print(len(answers), \"examples written\")\n",
        "  \n",
        "  writer.close()\n",
        "  \n",
        "  with tf.gfile.GFile(ANSWER_FILE, \"w\") as f:\n",
        "    f.write(\",\".join([str(i) + \"-\" + str(j) for i, j in zip(answers, localqid2globalqid)]) + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC0VCI1pYnp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "delete_tfflags([\n",
        "    \"vocab_file\", \"output_dir\", \"init_checkpoint\", \"max_seq_length\", \"do_train\", \"do_eval\", \n",
        "    \"train_batch_size\", \"eval_batch_size\", \"learning_rate\", \"save_checkpoints_steps\",\n",
        "    \"iterations_per_loop\", \"use_tpu\", \"tpu_name\", \"tpu_zone\", \"gcp_project\", \n",
        "    \"num_tpu_cores\", \"input_file\", \"output_dir\", \"max_predictions_per_seq\",\n",
        "    \"bert_config_file\", \"do_lower_case\", \"master\"\n",
        "])\n",
        "import run_classifier\n",
        "\n",
        "processor = None\n",
        "def make_tfrecord_eval(dst=TFRECORD_PRED_FILE):\n",
        "  global processor\n",
        "  processor = run_classifier.TltcProcessorWithGlobalTid()\n",
        "  \n",
        "  exists = !gsutil -q stat $TFRECORD_PRED_FILE; echo $?\n",
        "  if int(exists[0]) == 0:\n",
        "    print(\"eval tfrecord already exists. read it, with answer_file\")\n",
        "    \n",
        "    global answers, localqid2globalqid\n",
        "    answers = []\n",
        "    localqid2globalqid = []\n",
        "    with tf.gfile.GFile(ANSWER_FILE, \"r\") as f:\n",
        "      for expr in f.read().strip().split(\",\"):\n",
        "        tokens = expr.split(\"-\")\n",
        "        answers.append(int(tokens[0]))\n",
        "        localqid2globalqid.append(int(tokens[1]))\n",
        "    \n",
        "    return\n",
        "  else:\n",
        "    print(\"no eval tfrecord. make now\")\n",
        "  \n",
        "  tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=False, do_tokenize_chinese_chars=False, metawords=[\"[MASK]\", \"[UNK]\"]) \n",
        "  \n",
        "  pred_examples = processor.get_dev_examples_cv(get_addr_datasetdir(0), 0)\n",
        "  pred_features = run_classifier.convert_examples_to_features(pred_examples, label_list=processor.get_labels(), max_seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)\n",
        "  write_masklm_tfrecord_from_classifier_features(pred_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6AJMibDctsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def predict(prefix_resultfile=\"test\", checkpoint_path=None):\n",
        "  make_tfrecord_eval()\n",
        "  \n",
        "  print('***** Started prediction at {} *****'.format(datetime.datetime.now()))\n",
        "  \n",
        "  pred_input_fn = run_pretraining.input_fn_builder(\n",
        "        input_files=TFRECORD_PRED_FILE,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS_PER_SEQ,\n",
        "        is_training=False,\n",
        "        is_predicting=True)  \n",
        "  results = estimator.predict(input_fn=pred_input_fn, checkpoint_path=checkpoint_path)\n",
        "  \n",
        "  print('***** Finished prediction at {} *****'.format(datetime.datetime.now()))\n",
        "  \n",
        "  label_list = processor.get_labels()\n",
        "  \n",
        "  confmat = [[0 for _ in range(len(label_list))] for _ in range(len(label_list))]\n",
        "  counts = [0 for _ in range(len(label_list))]\n",
        "  \n",
        "  str_for_writer = \"\"\n",
        "  for i, result in enumerate(results):\n",
        "    if i % MAX_PREDICTIONS_PER_SEQ != 0:\n",
        "      continue\n",
        "    \n",
        "    if i//MAX_PREDICTIONS_PER_SEQ >= len(answers):\n",
        "      print(\"MAX_PRED_PASSED\")\n",
        "      break\n",
        "      \n",
        "    ans = answers[i//MAX_PREDICTIONS_PER_SEQ]\n",
        "    options = cm.get_choices_from_gtid(ans)\n",
        "    \n",
        "    probs = result[\"probabilities\"][options]\n",
        "    pred = cm.lid2gtids[cm.gtid2lid[ans]][np.argmax(probs)]\n",
        "    \n",
        "    str_for_writer += \"{}:{}-{},\".format(i, pred, ans)\n",
        "    \n",
        "    confmat[ans][pred] += 1\n",
        "    counts[ans] += 1\n",
        "  \n",
        "  with tf.gfile.GFile(get_name_resultfile(prefix_resultfile), \"w\") as writer:\n",
        "    writer.write(\"***** Writing start *****\\n\")\n",
        "    \n",
        "    print(\"accuracy of global check\")\n",
        "    ans_g = sum([v[i] for i, v in enumerate(confmat)])\n",
        "    total_g = sum([sum(v) for v in confmat])\n",
        "    print(\"accuracy {}% ({}/{})\".format(ans_g/total_g, ans_g, total_g))\n",
        "    writer.write(\"accuracy {}% ({}/{})\\n\".format(ans_g/total_g, ans_g, total_g))\n",
        "    \n",
        "    writer.write(\"ans|pred\\t#0\\t#1\\t#2\\n\")\n",
        "    for i, v in enumerate(confmat):\n",
        "      writer.write(\"#{}\\t{}\\n\".format(i, \"\\t\".join([str(n) for n in v])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wEif1QHI-Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_paths_from_steps(steps=0, used=500):\n",
        "  if steps is None or steps <= 0:\n",
        "    checkpoint_path = None\n",
        "    prefix_resultfile = \"iter{}_used{}\".format(0, used)\n",
        "  else:\n",
        "    checkpoint_path = \"{}/model.ckpt-{}\".format(MODEL_PATH, steps)\n",
        "    prefix_resultfile = \"iter{}_used{}\".format(steps, used)\n",
        "  \n",
        "  return prefix_resultfile, checkpoint_path\n",
        "\n",
        "estimator = make_estimator(init_checkpoint=INIT_CP, model_dir=MODEL_PATH)\n",
        "\n",
        "prefix_resultfile, checkpoint_path = make_paths_from_steps(steps=20000, used=500)\n",
        "predict(prefix_resultfile=prefix_resultfile, checkpoint_path=checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKXnYQLAgW7f",
        "colab_type": "text"
      },
      "source": [
        "## botu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIMbz33yqfwl",
        "colab_type": "text"
      },
      "source": [
        "### modify pretrained japanese bert model to treat legal terms as one word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5xoXxOHqgNT",
        "colab_type": "code",
        "outputId": "b862ecb4-d451-4d7e-f099-a465920895da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  saver = tf.train.Saver()\n",
        "  saver.restore(JPNBERT_PATH + \"/bert_model.ckpt.meta\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-cabca37a9c14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJPNBERT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/bert_model.ckpt.meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m    823\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m    824\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m    873\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m           build_restore=build_restore)\n\u001b[0m\u001b[1;32m    876\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[0;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[1;32m    488\u001b[0m       \u001b[0;31m# Add a placeholder string tensor for the filename.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       filename_tensor = array_ops.placeholder_with_default(\n\u001b[0;32m--> 490\u001b[0;31m           filename or \"model\", shape=(), name=\"filename\")\n\u001b[0m\u001b[1;32m    491\u001b[0m       \u001b[0;31m# Keep the name \"Const\" for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m       filename_tensor = array_ops.placeholder_with_default(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder_with_default\u001b[0;34m(input, shape, name)\u001b[0m\n\u001b[1;32m   2157\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \"\"\"\n\u001b[0;32m-> 2159\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder_with_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder_with_default\u001b[0;34m(input, shape, name)\u001b[0m\n\u001b[1;32m   6404\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6405\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6406\u001b[0;31m         \"PlaceholderWithDefault\", input=input, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6407\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6408\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    528\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                          as_ref=False):\n\u001b[1;32m    304\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   \"\"\"\n\u001b[1;32m    245\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 246\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    288\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[1;32m    289\u001b[0m              \"dtype\": dtype_value},\n\u001b[0;32m--> 290\u001b[0;31m       name=name).outputs[0]\n\u001b[0m\u001b[1;32m    291\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3586\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3588\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_finalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3589\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_check_not_finalized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3223\u001b[0m     \"\"\"\n\u001b[1;32m   3224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3225\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Graph is finalized and cannot be modified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Graph is finalized and cannot be modified."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQr0Kt1lObGR",
        "colab_type": "code",
        "outputId": "7333edd7-a120-4dcf-d6be-aae99411fe2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%env MODEL_PATH={MODEL_PATH}\n",
        "%env BERT_CONFIG_FILE={BERT_CONFIG_FILE}\n",
        "%env INIT_CP={INIT_CP}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: MODEL_PATH=gs://japan-legal-term-correction/pretrain_statutory_sentence/model\n",
            "env: BERT_CONFIG_FILE=gs://japan-legal-term-correction/bert/Japanese_L-12_H-768_A-12_E-30_BPE/bert_config.json\n",
            "env: INIT_CP=gs://japan-legal-term-correction/bert/Japanese_L-12_H-768_A-12_E-30_BPE/bert_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1l-Cos-OpZK",
        "colab_type": "code",
        "outputId": "3ca7a492-7f53-4c27-bf44-1e4af78c24ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%bash\n",
        "python bert-tltc/run_pretraining.py \\\n",
        "--input_file=${TFRECORD_FILE} --output_dir=${MODEL_PATH} \\\n",
        "--do_train=True --do_eval=True \\\n",
        "--bert_config_file=${BERT_CONFIG_FILE} --init_checkpoint=${INIT_CP} \\\n",
        "--train_batch_size=32 --max_seq_length=128 --max_predictions_per_seq=20 --num_train_steps=20 --num_warmup_steps=10 --learning_rate=2e-5 2>&1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process is terminated.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}